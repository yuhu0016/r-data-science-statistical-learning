---
title: "Hu.Yuanshan.Midterm"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(leaps)
library(ggplot2)
library(glmnet)
library(boot)
```

## Introduction
The goal of midterm is to apply some of the methods for supervised and unsupervised analysis to a new dataset. We will work with data characterizing the relationship between wine quality and its analytical characteristics available at UCI ML repository (Links to an external site.). The overall goal will be to use data modeling approaches to understand which wine properties influence the most wine quality as determined by expert evaluation. The output variable in this case assigns wine to discrete categories between 0 (the worst) and 10 (the best), so that this problem can be formulated as classification or regression – here we will stick to the latter and treat/model outcome as continuous variable. For more details please see dataset description available at UCI ML (Links to an external site.). Please note that there is another, much smaller, dataset on UCI ML also characterizing wine in terms of its analytical properties – make sure to use correct URL as shown above – the correct dataset contains several thousand observations.

There are two compilation of data available under the URL shown above – separate for red and for white wine – please develop models of wine quality for each of them, investigate attributes deemed important for wine quality in both and determine whether quality of red and white wine is influenced predominantly by the same or different analytical properties. Lastly, as an exercise in unsupervised learning you will be asked to combine analytical data for red and white wine and describe the structure of the resulting data – whether there are any well defined clusters, what subsets of observations they appear to represent, which attributes seem to affect the most this structure in the data, etc.


## Sub-problem 1: load and summarize the data (20 points)

Download and read in the data, produce numerical and graphical summaries of the dataset attributes, decide whether they can be used for modeling in untransformed form or any transformations are justified, comment on correlation structure and whether some of the predictors suggest relationship with the outcome.

```{r Sub-problem 1: load and summarize the data}
uciRoot <- "http://archive.ics.uci.edu/ml/machine-learning-databases/"
datURL <- "wine-quality/winequality-red.csv"
redWineQualityDat <- read.table(paste0(uciRoot ,datURL),sep=";", header= TRUE)
summary(redWineQualityDat)
#graphing pairs plot 
pairs(redWineQualityDat[1:11], pch =10, cex = 0.5, col=redWineQualityDat$quality, main = "RedWineQuality Pairs Plot")

for (i in 1:11){
  print ("pearson")
  print (cor(redWineQualityDat[i],redWineQualityDat[12], method="pearson"))
  print ("spearman")
  print (cor(redWineQualityDat[i],redWineQualityDat[12], method="spearman"))
}

logRedWineQualityDat <- log(redWineQualityDat +1)
pairs(logRedWineQualityDat[1:11], pch =10, cex = 0.5, col=logRedWineQualityDat$quality, main = "LogRedWineQuality Pairs Plot")

summary(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, redWineQualityDat))

summary(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, logRedWineQualityDat))

oldpar=par(mfrow=c(2,2))

plot(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, redWineQualityDat))

plot(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, logRedWineQualityDat))

par(oldpar)

uciRoot <- "http://archive.ics.uci.edu/ml/machine-learning-databases/"
datURL <- "wine-quality/winequality-white.csv"
whiteWineQualityDat <- read.table(paste0(uciRoot ,datURL),sep=";", header= TRUE)
summary(whiteWineQualityDat)
pairs(whiteWineQualityDat[1:11], pch =10, cex = 0.5, col=whiteWineQualityDat$quality, main = "WhiteWineQuality Pairs Plot")

for (i in 1:11){
  print ("pearson")
  print (cor(whiteWineQualityDat[i],whiteWineQualityDat[12], method="pearson"))
  print ("spearman")
  print (cor(whiteWineQualityDat[i],whiteWineQualityDat[12], method="spearman"))
}

logWhiteWineQualityDat <- log(whiteWineQualityDat +1)
for (i in 1:11){
  print ("pearson")
  print (cor(logWhiteWineQualityDat[i],logWhiteWineQualityDat[12], method="pearson"))
  print ("spearman")
  print (cor(logWhiteWineQualityDat[i],logWhiteWineQualityDat[12], method="spearman"))
}
pairs(logWhiteWineQualityDat[1:11], pch =10, cex = 0.5, col=logWhiteWineQualityDat$quality, main = "LogWhiteWineQuality Pairs Plot")

summary(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, whiteWineQualityDat))

summary(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, logWhiteWineQualityDat))

oldpar=par(mfrow=c(2,2))

plot(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, whiteWineQualityDat))

plot(lm(quality~fixed.acidity + volatile.acidity + citric.acid + residual.sugar+ +chlorides+ free.sulfur.dioxide+ total.sulfur.dioxide+ density+ pH+ sulphates+ alcohol, logWhiteWineQualityDat))

par(oldpar)
```

##Sub-problem 2: choose optimal models by exhaustive, forward and backward selection (20 points)

Use regsubsets from library leaps to choose optimal set of variables for modeling wine quality for red and white wine, describe differences and similarities between atributes deemed important in each case.

```{rChoosing Optimal Models by exhaustive, forward and backward selection}

##Red Wine Quality 
summaryMetrics <- NULL
whichAll <- list()
for ( myMthd in c("exhaustive", "backward", "forward") ) {
  rsRes <- regsubsets(quality~., redWineQualityDat,method=myMthd,nvmax=11)
  summRes <- summary(rsRes)
  whichAll[[myMthd]] <- summRes$which
  for ( metricName in c("rsq","rss","adjr2","cp","bic") ) {
    summaryMetrics <- rbind(summaryMetrics,
      data.frame(method=myMthd,metric=metricName,
                nvars=1:length(summRes[[metricName]]),
                value=summRes[[metricName]]))
  }
}
ggplot(summaryMetrics,aes(x=nvars,y=value,shape=method,colour=method)) + geom_path() + geom_point() + facet_wrap(~metric,scales="free") +   theme(legend.position="top")



```


```{r redWineQualityDat Which}
old.par <- par(mfrow=c(2,2),ps=16,mar=c(5,7,2,1))
for ( myMthd in names(whichAll) ) {
  image(1:nrow(whichAll[[myMthd]]),
        1:ncol(whichAll[[myMthd]]),
        whichAll[[myMthd]],xlab="N(vars)",ylab="",
        xaxt="n",yaxt="n",breaks=c(-0.5,0.5,1.5),
        col=c("white","gray"),main=myMthd)
  axis(1,1:nrow(whichAll[[myMthd]]),rownames(whichAll[[myMthd]]))
  axis(2,1:ncol(whichAll[[myMthd]]),colnames(whichAll[[myMthd]]),las=2)
}
par(old.par)
```

```{r}
#white wine 
summaryMetrics <- NULL
whichAll <- list()
for ( myMthd in c("exhaustive", "backward", "forward") ) {
  rsRes <- regsubsets(quality~., whiteWineQualityDat,method=myMthd,nvmax=11)
  summRes <- summary(rsRes)
  whichAll[[myMthd]] <- summRes$which
  for ( metricName in c("rsq","rss","adjr2","cp","bic") ) {
    summaryMetrics <- rbind(summaryMetrics,
      data.frame(method=myMthd,metric=metricName,
                nvars=1:length(summRes[[metricName]]),
                value=summRes[[metricName]]))
  }
}
ggplot(summaryMetrics,aes(x=nvars,y=value,shape=method,colour=method)) + geom_path() + geom_point() + facet_wrap(~metric,scales="free") +   theme(legend.position="top")

```

```{r whiteWineQualityDat Which}
old.par <- par(mfrow=c(2,2),ps=16,mar=c(5,7,2,1))
for ( myMthd in names(whichAll) ) {
  image(1:nrow(whichAll[[myMthd]]),
        1:ncol(whichAll[[myMthd]]),
        whichAll[[myMthd]],xlab="N(vars)",ylab="",
        xaxt="n",yaxt="n",breaks=c(-0.5,0.5,1.5),
        col=c("white","gray"),main=myMthd)
  axis(1,1:nrow(whichAll[[myMthd]]),rownames(whichAll[[myMthd]]))
  axis(2,1:ncol(whichAll[[myMthd]]),colnames(whichAll[[myMthd]]),las=2)
}
par(old.par)
```


## Sub-problem 3: optimal model by cross-validation (25 points)

Use cross-validation (or any other resampling strategy of your choice) to estimate test error for models
with different numbers of variables. Compare and comment on the number of variables deemed optimal
by resampling versus those selected by regsubsets in the previous task. Compare resulting models built
separately for red and white wine data.


##cross validation code (to be reviewed)

cv.error10=rep(0,11)

degree = 1:11
for ( d in degree) {
  glm.fit=glm(quality~poly(fixed.acidity, d), data = redWineQualityDat)
  cv.error10[d] = cv.glm(redWineQualityDat, glm.fit, K=10)$delta[1]

cv.error10
plot(degree,cv.error10,type="b", col="red")


```{r predictRegsubsets}
#red wine 
predict.regsubsets <- function (object, newdata, id, ...){
  form=as.formula(object$call [[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names (coefi)
  mat[,xvars] %*% coefi
}
```


```{rDrawTraining Sets}
#redwine
dfTmp <- NULL
whichSum <- array(0,dim=c(11,12,3),
  dimnames=list(NULL,colnames(model.matrix(quality~.,redWineQualityDat)),
      c("exhaustive", "backward", "forward")))
# Split data into training and test 30 times:
nTries <- 30
for ( iTry in 1:nTries ) {
  bTrainRedWine <- sample(rep(c(TRUE,FALSE),length.out=nrow(redWineQualityDat)))
  # Try each method available in regsubsets
  # to select best model of each size:
  for ( jSelect in c("exhaustive", "backward", "forward") ) {
    rsTrain <- regsubsets(quality~.,redWineQualityDat[bTrainRedWine,],nvmax=11,method=jSelect)
    # Add up variable selections:
    whichSum[,,jSelect] <- whichSum[,,jSelect] + summary(rsTrain)$which
    # Calculate test error for each set of variables
    # using predict.regsubsets implemented above:
    for ( kVarSet in 1:11 ) {
      # make predictions:
      testPred <- predict(rsTrain,redWineQualityDat[!bTrainRedWine,],id=kVarSet)
      # calculate MSE:
      mseTest <- mean((testPred-redWineQualityDat[!bTrainRedWine,"quality"])^2)
      # add to data.frame for future plotting:
      dfTmp <- rbind(dfTmp,data.frame(sim=iTry,sel=jSelect,vars=kVarSet,
      mse=c(mseTest,summary(rsTrain)$rss[kVarSet]/sum(bTrainRedWine)),trainTest=c("test","train")))
    }
  }
}
# plot MSEs by training/test, number of 
# variables and selection method:
ggplot(dfTmp,aes(x=factor(vars),y=mse,colour=sel)) + geom_boxplot()+facet_wrap(~trainTest)
```

```{r}
#red wine quality data
old.par <- par(mfrow=c(2,2),ps=16,mar=c(5,7,2,1))
for ( myMthd in dimnames(whichSum)[[3]] ) {
  tmpWhich <- whichSum[,,myMthd] / nTries
  image(1:nrow(tmpWhich),1:ncol(tmpWhich),tmpWhich,
        xlab="N(vars)",ylab="",xaxt="n",yaxt="n",main=myMthd,
        breaks=c(-0.1,0.1,0.25,0.5,0.75,0.9,1.1),
        col=c("white","gray90","gray75","gray50","gray25","gray10"))
  axis(1,1:nrow(tmpWhich),rownames(tmpWhich))
  axis(2,1:ncol(tmpWhich),colnames(tmpWhich),las=2)
}

```


```{rOptimal model by cross-validation}
#whitewine
predict.regsubsets.whitewine <- function (object, newdata, id, ...){
  form=as.formula(object$call [[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names (coefi)
  mat[,xvars] %*% coefi
}
```

```{rDrawTraining Sets}
#whitewine
dfTmp <- NULL
whichSum <- array(0,dim=c(11,12,3),
  dimnames=list(NULL,colnames(model.matrix(quality~.,whiteWineQualityDat)),
      c("exhaustive", "backward", "forward")))
# Split data into training and test 30 times:
nTries <- 30
for ( iTry in 1:nTries ) {
  bTrainWhiteWine <- sample(rep(c(TRUE,FALSE),length.out=nrow(whiteWineQualityDat)))
  # Try each method available in regsubsets
  # to select best model of each size:
  for ( jSelect in c("exhaustive", "backward", "forward") ) {
    rsTrain <- regsubsets(quality~.,whiteWineQualityDat[bTrainWhiteWine,],nvmax=11,method=jSelect)
    # Add up variable selections:
    whichSum[,,jSelect] <- whichSum[,,jSelect] + summary(rsTrain)$which
    # Calculate test error for each set of variables
    # using predict.regsubsets implemented above:
    for ( kVarSet in 1:11 ) {
      # make predictions:
      testPred <- predict(rsTrain,whiteWineQualityDat[!bTrainWhiteWine,],id=kVarSet)
      # calculate MSE:
      mseTest <- mean((testPred-whiteWineQualityDat[!bTrainWhiteWine,"quality"])^2)
      # add to data.frame for future plotting:
      dfTmp <- rbind(dfTmp,data.frame(sim=iTry,sel=jSelect,vars=kVarSet,
      mse=c(mseTest,summary(rsTrain)$rss[kVarSet]/sum(bTrainWhiteWine)),trainTest=c("test","train")))
    }
  }
}
# plot MSEs by training/test, number of 
# variables and selection method:
ggplot(dfTmp,aes(x=factor(vars),y=mse,colour=sel)) + geom_boxplot()+facet_wrap(~trainTest)
```

```{r}
#white wine quality data
old.par <- par(mfrow=c(2,2),ps=16,mar=c(5,7,2,1))
for ( myMthd in dimnames(whichSum)[[3]] ) {
  tmpWhich <- whichSum[,,myMthd] / nTries
  image(1:nrow(tmpWhich),1:ncol(tmpWhich),tmpWhich,
        xlab="N(vars)",ylab="",xaxt="n",yaxt="n",main=myMthd,
        breaks=c(-0.1,0.1,0.25,0.5,0.75,0.9,1.1),
        col=c("white","gray90","gray75","gray50","gray25","gray10"))
  axis(1,1:nrow(tmpWhich),rownames(tmpWhich))
  axis(2,1:ncol(tmpWhich),colnames(tmpWhich),las=2)
}
par=mfrow=c(1,1)
```

##Sub-problem 4: lasso/ridge (25 points)

Use regularized approaches (i.e. lasso and ridge) to model quality of red and white wine. Compare resulting models (in terms of number of variables and their effects) to those selected in the previous two tasks (by regsubsets and resampling), comment on differences and similarities among them. 

```{r}
#red wine Ridge
x <- model.matrix(quality~.,redWineQualityDat)[,-1]
y <- redWineQualityDat[,"quality"]
ridgeRes <- glmnet (x,y,alpha=0)
plot(ridgeRes, xlab="L2 Norm")

cvRidgeRes <- cv.glmnet(x,y,alpha=0)
plot(cvRidgeRes)

cvRidgeRes$lambda.min
cvRidgeRes$lambda.1se

predict(ridgeRes,type="coefficients",s=cvRidgeRes$lambda.1se)
```

```{r}
#red wine Lasso
lassoRes <- glmnet(x,y,alpha=1)
plot(lassoRes)
head(x)
cvLassoRes <- cv.glmnet(x,y,alpha=1)
plot(cvLassoRes)

predict(lassoRes,type="coefficients",s=cvLassoRes$lambda.1se)

predict(lassoRes,type="coefficients",s=cvLassoRes$lambda.min)

lassoCoefCnt <- 0
lassoMSE <- NULL
for ( iTry in 1:30 ) {
bTrainRedWine <- sample(rep(c(TRUE,FALSE),length.out=dim(x)[1]))
cvLassoTrain <- cv.glmnet(x[bTrainRedWine,],y[bTrainRedWine],alpha=1,lambda=10^((-120:0)/20))
lassoTrain <- glmnet(x[bTrainRedWine,],y[bTrainRedWine],alpha=1,lambda=10^((-120:0)/20))
lassoTrainCoef <- predict(lassoTrain,type="coefficients",s=cvLassoTrain$lambda.1se)
lassoCoefCnt <- lassoCoefCnt + (lassoTrainCoef[-1,1]!=0)
lassoTestPred <- predict(lassoTrain,newx=x[!bTrainRedWine,],s=cvLassoTrain$lambda.1se)
lassoMSE <- c(lassoMSE,mean((lassoTestPred-y[!bTrainRedWine])^2))
}
mean(lassoMSE)

lassoCoefCnt

```


```{r}
#white wine Ridge

x <- model.matrix(quality~.,whiteWineQualityDat)[,-1]
y <- whiteWineQualityDat[,"quality"]
ridgeRes <- glmnet (x,y,alpha=0)
plot(ridgeRes, xlab="L2 Norm")

cvRidgeRes <- cv.glmnet(x,y,alpha=0)
plot(cvRidgeRes)

cvRidgeRes$lambda.min
cvRidgeRes$lambda.1se

predict(ridgeRes,type="coefficients",s=cvRidgeRes$lambda.1se)
```


```{r}
#white wine Lasso
lassoRes <- glmnet(x,y,alpha=1)
plot(lassoRes)
head(x)
cvLassoRes <- cv.glmnet(x,y,alpha=1)
plot(cvLassoRes)

predict(lassoRes,type="coefficients",s=cvLassoRes$lambda.1se)

predict(lassoRes,type="coefficients",s=cvLassoRes$lambda.min)

lassoCoefCnt <- 0
lassoMSE <- NULL
for ( iTry in 1:30 ) {
bTrainWhiteWine <- sample(rep(c(TRUE,FALSE),length.out=dim(x)[1]))
cvLassoTrain <- cv.glmnet(x[bTrainWhiteWine,],y[bTrainWhiteWine],alpha=1,lambda=10^((-120:0)/20))
lassoTrain <- glmnet(x[bTrainWhiteWine,],y[bTrainWhiteWine],alpha=1,lambda=10^((-120:0)/20))
lassoTrainCoef <- predict(lassoTrain,type="coefficients",s=cvLassoTrain$lambda.1se)
lassoCoefCnt <- lassoCoefCnt + (lassoTrainCoef[-1,1]!=0)
lassoTestPred <- predict(lassoTrain,newx=x[!bTrainWhiteWine,],s=cvLassoTrain$lambda.1se)
lassoMSE <- c(lassoMSE,mean((lassoTestPred-y[!bTrainWhiteWine])^2))
}
mean(lassoMSE)

lassoCoefCnt

```

##Sub-problem 5: PCA (10 points)
Merge data for red and white wine (function rbind allows merging of two matrices/data frames with the same number of columns) and plot data projection to the first two principal components. Does this representation suggest presence of clustering structure in the data? Does wine type (red or white) or quality appear to be associated with different regions in the plot?

```{r}

```

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
